{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a895cf",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae0c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561a337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import timm.models.vision_transformer\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from timm.models.vision_transformer import PatchEmbed, Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e97b3c",
   "metadata": {},
   "source": [
    "## Food101 Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0acb3b",
   "metadata": {},
   "source": [
    "original train set: 75,750\n",
    "\n",
    "original validation set: 25,250\n",
    "\n",
    "total: 101,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2c7525",
   "metadata": {},
   "outputs": [],
   "source": [
    "food101 = load_dataset('food101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d876e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512>,\n",
       " 'label': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food101['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9a1138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75750/75750 [05:05<00:00, 248.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RGB: 0.5449871888617703, 0.4434935563380693, 0.34361316599832514\n",
      "Std RGB: 0.27093838406970966, 0.2734508551865403, 0.2780531622290323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "total_r = 0\n",
    "total_g = 0\n",
    "total_b = 0\n",
    "total_pixels = 0\n",
    "\n",
    "# Variables for standard deviation\n",
    "sum_squared_r = 0\n",
    "sum_squared_g = 0\n",
    "sum_squared_b = 0\n",
    "\n",
    "indices_to_drop = []\n",
    "\n",
    "for i in tqdm(range(len(food101['train']))):\n",
    "    img = np.array(food101['train'][i]['image'].resize((224, 224))) / 255.\n",
    "    \n",
    "    if img.shape == (224, 224, 3):\n",
    "        total_r += img[:, :, 0].sum()\n",
    "        total_g += img[:, :, 1].sum()\n",
    "        total_b += img[:, :, 2].sum()\n",
    "        \n",
    "        # For standard deviation\n",
    "        sum_squared_r += np.sum(np.square(img[:, :, 0]))\n",
    "        sum_squared_g += np.sum(np.square(img[:, :, 1]))\n",
    "        sum_squared_b += np.sum(np.square(img[:, :, 2]))\n",
    "\n",
    "        total_pixels += 224 * 224\n",
    "    else:\n",
    "        indices_to_drop.append(i)\n",
    "\n",
    "mean_r = total_r / total_pixels\n",
    "mean_g = total_g / total_pixels\n",
    "mean_b = total_b / total_pixels\n",
    "\n",
    "# Compute std for each channel\n",
    "std_r = np.sqrt((sum_squared_r / total_pixels) - (mean_r ** 2))\n",
    "std_g = np.sqrt((sum_squared_g / total_pixels) - (mean_g ** 2))\n",
    "std_b = np.sqrt((sum_squared_b / total_pixels) - (mean_b ** 2))\n",
    "\n",
    "print(f\"Mean RGB: {mean_r}, {mean_g}, {mean_b}\")\n",
    "print(f\"Std RGB: {std_r}, {std_g}, {std_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce42ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ebe6c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "food101_mean = np.array([mean_r, mean_g, mean_b])\n",
    "food101_std = np.array([std_r, std_g, std_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177d2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    \"\"\"\n",
    "    reshape the images into 224 * 224\n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    \n",
    "    pixel_values = []\n",
    "    labels = []\n",
    "    for i in range(len(example_batch['image'])):\n",
    "        x = example_batch['image'][i]\n",
    "        y = example_batch['label'][i]\n",
    "        if np.array(x.resize((224, 224))).shape == (224, 224, 3):\n",
    "            pixel_values.append(torch.tensor(((np.array(x.resize((224, 224))) / 255. - food101_mean) / food101_std), dtype = torch.float).permute(2, 0, 1))\n",
    "            labels.append(y)\n",
    "\n",
    "    inputs['pixel_values'] = pixel_values\n",
    "    inputs['label'] = labels\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91632b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(food101['train'][0]['image'].resize((224, 224))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7888bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_food101 = food101.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ffbf30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = processed_food101['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9952eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_choose = list(set(range(len(train_dataset))) - set(indices_to_drop))\n",
    "filtered_train_dataset = train_dataset.select(indices_to_choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2e5e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = processed_food101['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65c9c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_choose = list(set(range(len(validation_dataset))) - set(indices_to_drop))\n",
    "filtered_valid_dataset = validation_dataset.select(indices_to_choose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acaa800",
   "metadata": {},
   "source": [
    "## ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b478a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7c1b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=101):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.in_planes = 128\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 128, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 512, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 1024, num_blocks[3], stride=2)\n",
    "\n",
    "        self.linear = nn.Linear(1024 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        # Global average pooling\n",
    "        out = F.avg_pool2d(out, kernel_size=out.size()[2:])\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c493d",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "017076cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20a76bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_epoch(clf,\n",
    "                   train_dataset,\n",
    "                   batch_size=128,\n",
    "                   lr=5e-5,\n",
    "                   device=\"cuda:0\"):\n",
    "    clf.train()\n",
    "    loader = DataLoader(train_dataset, batch_size, drop_last=True, shuffle = True)\n",
    "    \n",
    "    params_to_update = []\n",
    "    for name, param in clf.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params_to_update, lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    loss_list = []\n",
    "    avg_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Train\"):\n",
    "        # Train loop.\n",
    "        optimizer.zero_grad()\n",
    "        cls_logit = clf(batch['pixel_values'].to(device))\n",
    "        \n",
    "        loss_cls = criterion(cls_logit, batch['label'].to(device).squeeze())\n",
    "        \n",
    "        loss = loss_cls\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # Have gradients at this point.\n",
    "        nn.utils.clip_grad_norm_(clf.parameters(), max_norm=5.0, norm_type=2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6444dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(clf, eval_dataset, batch_size):\n",
    "    clf.eval()\n",
    "    loader = DataLoader(eval_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    n_right_classes = 0\n",
    "    n_total = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Eval\"):\n",
    "        # Compute accuracy.\n",
    "        cls_logit = clf(batch['pixel_values'].to(device))\n",
    "        \n",
    "        pred = cls_logit.argmax(dim=1)\n",
    "        \n",
    "        n_right_classes_batch = sum(pred == batch['label'].to(device)).item()\n",
    "        \n",
    "        n_right_classes += n_right_classes_batch\n",
    "        \n",
    "        n_total += pred.numel()\n",
    "\n",
    "    print(\"  Acc_cls:\", n_right_classes / n_total)\n",
    "\n",
    "    return n_right_classes / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0a27f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(clf, train_dataset, test_dataset, n_epochs: int = 1, model_name=None, **args):\n",
    "    print(\"Using device:\", args[\"device\"])\n",
    "    train = train_dataset\n",
    "\n",
    "    valid = test_dataset\n",
    "    loss = []\n",
    "    acc = []\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Starting epoch {epoch+1}...\")\n",
    "        loss_list = finetune_epoch(clf, train, **args)\n",
    "        loss += loss_list\n",
    "\n",
    "        # Save the final checkpoints of the model\n",
    "        if model_name is not None:\n",
    "            torch.save(clf, model_path + model_name + 'epoch_' + str(epoch+1) + '.pt')\n",
    "\n",
    "        acc_i = evaluate(clf, valid, 32)\n",
    "        acc.append(acc_i)\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bde3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [1:02:45<00:00,  1.26it/s]\n",
      "Eval: 100%|██████████| 789/789 [09:29<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.1806479721166033\n",
      "Starting epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [57:42<00:00,  1.37it/s] \n",
      "Eval: 100%|██████████| 789/789 [08:00<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.3320659062103929\n",
      "Starting epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [57:38<00:00,  1.37it/s] \n",
      "Eval: 100%|██████████| 789/789 [07:17<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.37828738910012677\n",
      "Starting epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [53:03<00:00,  1.49it/s] \n",
      "Eval: 100%|██████████| 789/789 [07:21<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.49081115335868186\n",
      "Starting epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [52:59<00:00,  1.49it/s]\n",
      "Eval: 100%|██████████| 789/789 [07:43<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.5135852344740177\n",
      "Starting epoch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [52:50<00:00,  1.49it/s]\n",
      "Eval: 100%|██████████| 789/789 [07:17<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.5878089353612167\n",
      "Starting epoch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [53:25<00:00,  1.48it/s]\n",
      "Eval: 100%|██████████| 789/789 [07:37<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.5878485424588086\n",
      "Starting epoch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [1:01:33<00:00,  1.28it/s]\n",
      "Eval: 100%|██████████| 789/789 [09:58<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.597671102661597\n",
      "Starting epoch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [1:06:38<00:00,  1.18it/s]\n",
      "Eval: 100%|██████████| 789/789 [11:01<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.6082461977186312\n",
      "Starting epoch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [1:01:25<00:00,  1.28it/s]\n",
      "Eval: 100%|██████████| 789/789 [09:20<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.641318124207858\n",
      "Starting epoch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [1:04:16<00:00,  1.23it/s]\n",
      "Eval: 100%|██████████| 789/789 [09:36<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.6550617870722434\n",
      "Starting epoch 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [57:22<00:00,  1.38it/s] \n",
      "Eval: 100%|██████████| 789/789 [09:40<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.6824302915082383\n",
      "Starting epoch 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [1:01:30<00:00,  1.28it/s]\n",
      "Eval: 100%|██████████| 789/789 [09:10<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.6826679340937896\n",
      "Starting epoch 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [1:02:42<00:00,  1.26it/s]\n",
      "Eval: 100%|██████████| 789/789 [10:33<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.6680529150823827\n",
      "Starting epoch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 4734/4734 [1:02:16<00:00,  1.27it/s]\n",
      "Eval: 100%|██████████| 789/789 [10:21<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.653041825095057\n",
      "Starting epoch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  20%|█▉        | 932/4734 [11:58<43:17,  1.46it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Train: 100%|██████████| 4734/4734 [1:00:50<00:00,  1.30it/s]\n",
      "Eval: 100%|██████████| 789/789 [10:39<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acc_cls: 0.680410329531052\n",
      "Starting epoch 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  39%|███▉      | 1864/4734 [24:43<41:37,  1.15it/s] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 20\n",
    "model = ResNet50(Bottleneck, [3, 4, 6, 3]).to(device)\n",
    "classifier = model\n",
    "classifier.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5, betas=(0.9, 0.95), weight_decay=0.05)\n",
    "\n",
    "loss = finetune(classifier, filtered_train_dataset, filtered_valid_dataset, EPOCHS, batch_size=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25219a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1806479721166033,\n",
       " 0.3320659062103929,\n",
       " 0.37828738910012677,\n",
       " 0.49081115335868186,\n",
       " 0.5135852344740177,\n",
       " 0.5878089353612167,\n",
       " 0.5878485424588086,\n",
       " 0.597671102661597,\n",
       " 0.6082461977186312,\n",
       " 0.641318124207858,\n",
       " 0.6550617870722434,\n",
       " 0.6824302915082383,\n",
       " 0.6826679340937896,\n",
       " 0.6680529150823827,\n",
       " 0.653041825095057,\n",
       " 0.6863513941698353,\n",
       " 0.7082145120405576,\n",
       " 0.680410329531052,\n",
       " 0.6879356780735107,\n",
       " 0.6656764892268695]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65efe5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
